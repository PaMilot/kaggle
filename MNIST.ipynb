{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1df98c716f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# https://www.kaggle.com/competitions/digit-recognizer/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data:\n",
    "    \"\"\"\n",
    "    A class for handling the creation of the train, validation and test dataset, pytorch ready.\n",
    "\n",
    "    train_path : string\n",
    "        path for the train images\n",
    "    test_path : string\n",
    "        path for the test images\n",
    "    batch_size : int\n",
    "    train_split_ration : float [0, 1]\n",
    "        ratio of training images in the train image folder.\n",
    "        The rest of the images will be allocated to the valdiation dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path, test_path, batch_size, train_split_ratio):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.batch_size = batch_size\n",
    "        self.train_split_ratio = train_split_ratio\n",
    "\n",
    "    class MNIST_train(Dataset):\n",
    "        def __init__(self, pixels, labels):\n",
    "            self.pixels = torch.tensor(pixels, dtype=torch.float32).view(-1, 1, 28, 28) / 255.0  # Normalize to [0, 1]\n",
    "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.pixels[idx], self.labels[idx]\n",
    "\n",
    "    class MNIST_test(Dataset):\n",
    "        def __init__(self, pixels, test_df):\n",
    "            self.pixels = torch.tensor(pixels, dtype=torch.float32).view(-1, 1, 28, 28) / 255.0  # Normalize to [0, 1]\n",
    "            self.test_df = test_df\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.test_df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.pixels[idx]\n",
    "\n",
    "\n",
    "    def make_train(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Initializes train and validation datasets.\n",
    "\n",
    "        RETURN : Dataloader\n",
    "            train_loader\n",
    "            val_loader\n",
    "        \"\"\"\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        train_df = pd.read_csv(self.train_path)\n",
    "        labels = train_df['label'].values\n",
    "        pixels = train_df.drop(columns=['label']).values\n",
    "        mnist_dataset = self.MNIST_train(pixels, labels)\n",
    "\n",
    "        train_size = int(self.train_split_ratio * len(mnist_dataset))\n",
    "        val_size = len(mnist_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def make_test(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Initializes a test dataset to later get a Kaggle score for the competition.\n",
    "\n",
    "        RETURN : Dataloader\n",
    "            test_laoder\n",
    "        \"\"\"\n",
    "        batch_size = batch_size or self.batch_size # should not be necessary as test datas are not used in the grid_search\n",
    "        test_df = pd.read_csv(self.test_path)\n",
    "        pixels = test_df.values\n",
    "\n",
    "        mnist_dataset_test = self.MNIST_test(pixels, test_df)\n",
    "        test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    \"\"\"\n",
    "    A class for handling the training, evaluation and submission of the MNIST kaggle challenge.\n",
    "\n",
    "    mnist_data : MNIST_data\n",
    "        An instance of MNIST_data containing both train, validation and test dataset (pytorch ready)\n",
    "    \"\"\"\n",
    "    def __init__(self, mnist_data):\n",
    "        self.mnist_data = mnist_data\n",
    "\n",
    "        # loaders :\n",
    "        self.train_loader, self.val_loader = mnist_data.make_train()\n",
    "        self.test_loader = mnist_data.make_test()\n",
    "\n",
    "    def train(self, model, criterion, optimizer, epochs=10, verbose = 0):\n",
    "        \"\"\"\n",
    "        Basic PyTorch model training algorithm.\n",
    "\n",
    "        model     : torch.nn.Module\n",
    "        criterion : torch.nn.Module\n",
    "        optimizer : torch.optim.Optimizer\n",
    "        epochs    : int\n",
    "        verbose   : int, optional\n",
    "            0 recommended when performing a GridSearch.\n",
    "\n",
    "        RETURN : dict\n",
    "            dictionary containing the training loss for each batch.\n",
    "        \"\"\"\n",
    "        output = {'training_loss': []}  \n",
    "        for epoch in range(epochs):\n",
    "            if verbose : print(str(epoch) + \" / \" + str(epochs))\n",
    "            for i, (image, pred) in enumerate(self.train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                z = model(image)\n",
    "                loss = criterion(z, pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                output['training_loss'].append(loss.data.item())\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def evaluation(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate the model's performance on the validation dataset.\n",
    "\n",
    "        model : torch.nn.Module\n",
    "\n",
    "        RETURN : float\n",
    "            accuracy score on the validation set.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for img, label in self.val_loader:\n",
    "            for i in range(len(label)):\n",
    "                if model(img[i]).argmax() == label[i] :\n",
    "                    count = count+1\n",
    "        return count/(len(self.val_loader)*len(label))\n",
    "    \n",
    "    \n",
    "\n",
    "    def submit(self, model):\n",
    "        \"\"\"\n",
    "        Generates predictions for the test set and saves them to a CSV file\n",
    "        \n",
    "        model : torch.nn.Module\n",
    "        \n",
    "        RETURN : None\n",
    "            Saves the predictions to \"submission.csv\" (needs to be in the python file directory) formated to kaggle's\n",
    "            submission standards.\n",
    "        \"\"\"\n",
    "        f = open(\"submission.csv\", \"a\")\n",
    "        f.write(\"ImageId,Label\\n\")\n",
    "        i = 1\n",
    "        for x in self.test_loader:\n",
    "            batch_pred = model(x)\n",
    "            for elt in batch_pred:\n",
    "                f.write(str(str(i) + \",\" + str(elt.argmax().numpy()) + \"\\n\"))\n",
    "                i = i + 1\n",
    "        f.close()\n",
    "        print(\"File created, ready to submit.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_gridSearch:\n",
    "    \"\"\"\n",
    "        A class to perform a gridsearch (as seen in SciKit) on a pytorch model\n",
    "        over the following hyperparameters :\n",
    "            model          : torch.nn.Module \n",
    "            mnist          : MNIST\n",
    "            criterions     : list of torch.nn.Module\n",
    "            optimizers     : list of torch.optim.Optimizer\n",
    "            epochs         : list of int\n",
    "            learning_rates : list of float\n",
    "            batch_sizes    : list of int\n",
    "        \n",
    "        The optimizers and criterions need to be able to work without changing the model's dimension\n",
    "        or it will raise an error and stop the program\n",
    "    \"\"\"\n",
    "    def __init__(self, model, mnist: MNIST, criterions, optimizers,\n",
    "                 epochs = [10],\n",
    "                 learning_rates = [0.001],\n",
    "                 batch_sizes = [32]):\n",
    "        \"\"\"\n",
    "        Initializes the grid search configuration.\n",
    "        \n",
    "        total_iterations : int\n",
    "            amount of possible combiantions for the input parameters.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.mnist = mnist\n",
    "        self.criterions = criterions\n",
    "        self.optimizers = optimizers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rates = learning_rates\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.total_iterations = len(criterions) * len(optimizers) * len(learning_rates) * len(batch_sizes) * len(epochs)\n",
    "\n",
    "    def gridSearch(self, verbose = 0) :\n",
    "        \"\"\"\n",
    "        Performs a GridSearch over specified hyperparameters. Every combination of hyperparameters will be tested,\n",
    "        this can be a very long process\n",
    "        \n",
    "        verbose : int\n",
    "            set to 1 to display (current_iteration)/(toral_iterations) during the process. (highly recommended)\n",
    "\n",
    "        RETURN : list\n",
    "            Outputs the list of the best hyperparameters combination in the provided grid and its score.\n",
    "            [optimizer, criterion, epoch, learning_rate, batch_size, score]\n",
    "        \n",
    "        \"\"\"\n",
    "        max = [0, 0, 0, 0, 0, 0] # opt, crit, epoch, l_rate, batch_size, score\n",
    "        iteration = 0\n",
    "        for batch_size in self.batch_sizes:\n",
    "            self.mnist.train_loader, self.mnist.val_loader = self.mnist.mnist_data.make_train(batch_size)\n",
    "            for optimizer in self.optimizers:\n",
    "                for criterion in self.criterions:\n",
    "                    for epoch in self.epochs:\n",
    "                        for l_rate in self.learning_rates:\n",
    "                            iteration += 1\n",
    "                            optim = optimizer(self.model.parameters(), lr=l_rate)\n",
    "                            self.mnist.train(self.model, criterion(), optim, epoch)\n",
    "\n",
    "                            score = self.mnist.evaluation(self.model)\n",
    "\n",
    "                            if verbose >= 1 : print(f\"Iteration {iteration} / {self.total_iterations} : score : {score}\")\n",
    "                            if score > max[5] :\n",
    "                                max = [optimizer, criterion, epoch, l_rate, batch_size, score]\n",
    "        \n",
    "        return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaulNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PaulNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride = 1, padding = 0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(12*12*8, 56)\n",
    "        self.fc2 = nn.Linear(56, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.pool1(out)\n",
    "        out = out.view(-1, 12*12*8)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        # out = F.softmax(out, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 / 27 : score : 0.9646428571428571\n",
      "Iteration 2 / 27 : score : 0.9783333333333334\n",
      "Iteration 3 / 27 : score : 0.983452380952381\n",
      "Iteration 4 / 27 : score : 0.9704761904761905\n",
      "Iteration 5 / 27 : score : 0.97\n",
      "Iteration 6 / 27 : score : 0.9864285714285714\n",
      "Iteration 7 / 27 : score : 0.9736904761904762\n",
      "Iteration 8 / 27 : score : 0.9763095238095238\n",
      "Iteration 9 / 27 : score : 0.985\n",
      "Iteration 10 / 27 : score : 0.9850906488549618\n",
      "Iteration 11 / 27 : score : 0.9846135496183206\n",
      "Iteration 12 / 27 : score : 0.9909351145038168\n",
      "Iteration 13 / 27 : score : 0.9805582061068703\n",
      "Iteration 14 / 27 : score : 0.9873568702290076\n",
      "Iteration 15 / 27 : score : 0.9893845419847328\n",
      "Iteration 16 / 27 : score : 0.981631679389313\n",
      "Iteration 17 / 27 : score : 0.984375\n",
      "Iteration 18 / 27 : score : 0.987118320610687\n",
      "Iteration 19 / 27 : score : 0.9904580152671756\n",
      "Iteration 20 / 27 : score : 0.9927242366412213\n",
      "Iteration 21 / 27 : score : 0.9958253816793893\n",
      "Iteration 22 / 27 : score : 0.9860448473282443\n",
      "Iteration 23 / 27 : score : 0.9914122137404581\n",
      "Iteration 24 / 27 : score : 0.9937977099236641\n",
      "Iteration 25 / 27 : score : 0.9877146946564885\n",
      "Iteration 26 / 27 : score : 0.9885496183206107\n",
      "Iteration 27 / 27 : score : 0.991770038167939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.optim.adamw.AdamW,\n",
       " torch.nn.modules.loss.CrossEntropyLoss,\n",
       " 1,\n",
       " 0.0005,\n",
       " 64,\n",
       " 0.9958253816793893]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = [16, 32, 64]\n",
    "lrs = [0.01, 0.007, 0.0005]\n",
    "epochs = [1, 2, 3]\n",
    "model = PaulNet()\n",
    "criterions = [nn.CrossEntropyLoss]\n",
    "optimizers = [optim.AdamW]\n",
    "\n",
    "data = MNIST_data('train.csv', 'test.csv', 32, 0.8)\n",
    "mnist = MNIST(data)\n",
    "grid_search = MNIST_gridSearch(model, mnist, criterions, optimizers, epochs, lrs, batch_sizes)\n",
    "\n",
    "best = grid_search.gridSearch(verbose = 1)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [torch.optim.adamw.AdamW,\n",
    "#  torch.nn.modules.loss.CrossEntropyLoss,\n",
    "#  1,\n",
    "#  0.0005,\n",
    "#  64,\n",
    "#  0.9945133587786259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
